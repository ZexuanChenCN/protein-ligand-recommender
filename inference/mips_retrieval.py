"""
Protein-Ligandæ£€ç´¢ç³»ç»Ÿ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
æ ¸å¿ƒæé€Ÿç‚¹ï¼š
1. è›‹ç™½ç¼–ç å‘é‡åŒ–ï¼ˆæ›¿æ¢æ‰‹åŠ¨listå¾ªç¯ï¼‰
2. å¼€å¯æ¨ç†æ¨¡å¼+æ··åˆç²¾åº¦
3. æ‰¹é‡å¤„ç†ä¼˜åŒ–
4. é¢„ç¼–è¯‘æ¨¡å‹
"""
import torch
import numpy as np
import warnings
from tqdm import tqdm
import faiss
import os
import sys
from pathlib import Path
warnings.filterwarnings("ignore")

sys.path.append(os.path.dirname(os.path.abspath(__file__)))


# å¼€å¯æ··åˆç²¾åº¦æ¨ç†ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision('high')


import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer, AutoConfig

class SaProtProteinEncoder(nn.Module):
    """SaProtè›‹ç™½ç¼–ç å™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
    def __init__(self, saprot_model_dir, proj_dim=256, dropout=0.2, freeze_backbone=True):
        super().__init__()
        self.saprot_model_dir = Path(saprot_model_dir).resolve()
        print(f"ğŸ“Œ åŠ è½½æœ¬åœ°SaProtæ¨¡å‹ç»“æ„ï¼š{self.saprot_model_dir}")

        config_path = self.saprot_model_dir / "config.json"
        if not config_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°SaProté…ç½®æ–‡ä»¶ï¼š{config_path}")

        self.config = AutoConfig.from_pretrained(
            str(config_path),
            local_files_only=True,
            ignore_mismatched_sizes=True
        )

        self.bert = AutoModel.from_pretrained(
            str(self.saprot_model_dir),
            config=self.config,
            local_files_only=True,
            ignore_mismatched_sizes=True
        )

        if freeze_backbone:
            for param in self.bert.parameters():
                param.requires_grad = False

        self.proj = nn.Sequential(
            nn.Linear(self.config.hidden_size, proj_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(proj_dim, proj_dim)
        )
        self.dropout = nn.Dropout(dropout)

        self.aa_vocab = {'A':0, 'C':1, 'D':2, 'E':3, 'F':4, 'G':5, 'H':6,
                         'I':7, 'K':8, 'L':9, 'M':10, 'N':11, 'P':12, 'Q':13,
                         'R':14, 'S':15, 'T':16, 'V':17, 'W':18, 'Y':19, 'X':20}
        self.vocab_size = len(self.aa_vocab)
        self.max_len = 1024

    def tokenize_batch_vectorized(self, seqs):
        """å‘é‡åŒ–æ‰¹é‡tokenize"""
        seqs = [seq.upper()[:self.max_len] for seq in seqs]
        seq_ids = []
        for seq in seqs:
            ids = [self.aa_vocab.get(c, 20) for c in seq if c in self.aa_vocab]
            seq_ids.append(ids)

        max_batch_len = min(max(len(ids) for ids in seq_ids), self.max_len)
        input_ids = torch.zeros((len(seqs), max_batch_len), dtype=torch.long)
        attention_masks = torch.zeros((len(seqs), max_batch_len), dtype=torch.long)

        for i, ids in enumerate(seq_ids):
            valid_len = min(len(ids), max_batch_len)
            input_ids[i, :valid_len] = torch.tensor(ids[:valid_len])
            attention_masks[i, :valid_len] = 1

        return input_ids, attention_masks

    def forward(self, seqs):
        """å‰å‘ä¼ æ’­ï¼ˆ"""
        input_ids, attention_masks = self.tokenize_batch_vectorized(seqs)

        input_ids = input_ids.to(self.bert.device)
        attention_masks = attention_masks.to(self.bert.device)

        with torch.inference_mode():
            bert_out = self.bert(input_ids=input_ids, attention_mask=attention_masks)
            cls_out = bert_out.last_hidden_state[:, 0, :]
            cls_out = self.dropout(cls_out)
            proj_out = self.proj(cls_out)

        return F.normalize(proj_out, p=2, dim=-1)

class ChemBERTaEncoder(nn.Module):
    """ChemBERTaç¼–ç å™¨"""
    def __init__(self, chemberta_model_name, proj_dim=256, dropout=0.2, freeze_backbone=True):
        super().__init__()
        print(f"ğŸ“Œ åŠ è½½ChemBERTaæ¨¡å‹ï¼š{chemberta_model_name}")

        self.bert = AutoModel.from_pretrained(
            chemberta_model_name,
            local_files_only=False,
            trust_remote_code=True,
            ignore_mismatched_sizes=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            chemberta_model_name,
            local_files_only=False,
            trust_remote_code=True
        )

        if freeze_backbone:
            for param in self.bert.parameters():
                param.requires_grad = False

        self.proj = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, proj_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(proj_dim, proj_dim)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, smiles_list):
        """å‰å‘ä¼ æ’­"""
        with torch.inference_mode():
            inputs = self.tokenizer(
                smiles_list, padding=True, truncation=True, max_length=128,
                return_tensors="pt", return_attention_mask=True
            ).to(self.bert.device)

            bert_out = self.bert(**inputs)
            cls_out = bert_out.last_hidden_state[:, 0, :]
            cls_out = self.dropout(cls_out)
            proj_out = self.proj(cls_out)

        return F.normalize(proj_out, p=2, dim=-1)

class CLIPStyleDualTower(nn.Module):
    """åŒå¡”æ¨¡å‹"""
    def __init__(self, saprot_model_dir, chemberta_model_name,
                 proj_dim=256, init_temperature=0.2, dropout=0.2,
                 freeze_saprot=True, freeze_chemberta=True):
        super().__init__()
        self.protein_encoder = SaProtProteinEncoder(
            saprot_model_dir=saprot_model_dir,
            proj_dim=proj_dim,
            dropout=dropout,
            freeze_backbone=freeze_saprot
        )
        self.ligand_encoder = ChemBERTaEncoder(
            chemberta_model_name=chemberta_model_name,
            proj_dim=proj_dim,
            dropout=dropout,
            freeze_backbone=freeze_chemberta
        )

        self.temperature = torch.nn.Parameter(torch.tensor(init_temperature))
        self.proj_dim = proj_dim

    def encode_protein(self, protein_seqs):
        return self.protein_encoder(protein_seqs)

    def encode_ligand(self, ligand_smiles):
        return self.ligand_encoder(ligand_smiles)

    def forward(self, protein_seqs, ligand_smiles, neg_ligand_smiles=None):
        protein_embs = self.encode_protein(protein_seqs)
        ligand_embs = self.encode_ligand(ligand_smiles)
        sim = torch.matmul(protein_embs, ligand_embs.t()) / self.temperature

        if neg_ligand_smiles is not None:
            neg_ligand_embs = self.encode_ligand(neg_ligand_smiles)
            neg_sim = torch.matmul(protein_embs, neg_ligand_embs.t()) / self.temperature
            return sim, neg_sim
        return sim

    @classmethod
    def load_from_checkpoint(cls, checkpoint_path, **kwargs):
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        print(f"ğŸ“Œ ä»ckptåŠ è½½è®­ç»ƒå‚æ•°ï¼š{checkpoint_path}")

        hparams = checkpoint.get("hyper_parameters", {})
        load_kwargs = {
            "saprot_model_dir": kwargs.get("saprot_model_dir", hparams.get("saprot_model_dir")),
            "chemberta_model_name": kwargs.get("chemberta_model_name", hparams.get("chemberta_model_name")),
            "proj_dim": kwargs.get("proj_dim", hparams.get("proj_dim", 256)),
            "init_temperature": kwargs.get("init_temperature", hparams.get("init_temperature", 0.2)),
            "dropout": kwargs.get("dropout", hparams.get("dropout", 0.2)),
            "freeze_saprot": kwargs.get("freeze_saprot", hparams.get("freeze_saprot", True)),
            "freeze_chemberta": kwargs.get("freeze_chemberta", hparams.get("freeze_chemberta", True))
        }

        model = cls(**load_kwargs)
        missing_keys, unexpected_keys = model.load_state_dict(
            checkpoint["state_dict"],
            strict=False
        )
        print(f"âš ï¸  æƒé‡åŠ è½½ç»Ÿè®¡ - ç¼ºå¤±keys: {len(missing_keys)} | é¢å¤–keys: {len(unexpected_keys)}")

        # æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼Œå¤§å¹…æé€Ÿï¼‰
        if torch.__version__ >= "2.0.0":
            model = torch.compile(model)

        return model

class ProteinLigandRetriever:
    def __init__(self, checkpoint_path, saprot_model_dir, chemberta_model_name,
                 device="cuda:0", temperature_scale=0.5, batch_size=64):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        print(f"æ£€ç´¢å™¨åˆå§‹åŒ–è®¾å¤‡ï¼š{self.device}")

        # åŠ è½½æ¨¡å‹
        self.model = CLIPStyleDualTower.load_from_checkpoint(
            checkpoint_path=checkpoint_path,
            saprot_model_dir=saprot_model_dir,
            chemberta_model_name=chemberta_model_name,
            freeze_saprot=True,
            freeze_chemberta=True
        ).to(self.device)

        # æè‡´æ¨ç†æ¨¡å¼
        self.model.eval()
        self.model = self.model.half()  # æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16ï¼‰

        self.model.temperature.data = torch.tensor(temperature_scale).to(self.device).half()
        print(f"âœ… æ¸©åº¦ç³»æ•°è®¾ç½®ä¸ºï¼š{self.model.temperature.item():.8f}")

        # æ‰¹é‡å¤§å°ä¼˜åŒ–
        self.batch_size = batch_size
        self.protein_index = None
        self.ligand_index = None
        self.protein_id2seq = {}
        self.ligand_id2smiles = {}
        self.raw_dataset_samples = []

    def preprocess_protein(self, seq):
        """å¿«é€Ÿé¢„å¤„ç†"""
        aa_vocab = {'A':0, 'C':1, 'D':2, 'E':3, 'F':4, 'G':5, 'H':6,
                    'I':7, 'K':8, 'L':9, 'M':10, 'N':11, 'P':12, 'Q':13,
                    'R':14, 'S':15, 'T':16, 'V':17, 'W':18, 'Y':19, 'X':20}
        max_len = 1024
        seq = seq.upper()
        seq = ''.join([c for c in seq if c in aa_vocab.keys()])
        return seq[:max_len]

    def preprocess_ligand(self, smiles):
        if not smiles or len(smiles) < 1:
            return "C"
        return smiles.replace(' ', '').lower()

    def encode_protein_batch(self, protein_seqs):
        """æ‰¹é‡ç¼–ç """
        processed_seqs = [self.preprocess_protein(seq) for seq in protein_seqs]
        with torch.no_grad():
            # æ··åˆç²¾åº¦æ¨ç†
            protein_emb = self.model.encode_protein(processed_seqs).float()
        return protein_emb.cpu().numpy()

    def encode_ligand_batch(self, ligand_smiles):
        """æ‰¹é‡ç¼–ç """
        processed_smiles = [self.preprocess_ligand(smi) for smi in ligand_smiles]
        with torch.no_grad():
            ligand_emb = self.model.encode_ligand(processed_smiles).float()
        return ligand_emb.cpu().numpy()

    def build_indexes_from_dataset(self, dataset, max_proteins=2000, max_ligands=8000):
        print("ğŸ“¥ åŠ è½½æ•°æ®é›†ï¼šBALM/BALM-benchmark - BindingDB_filtered")
        dataset = dataset["train"]

        def filter_invalid(sample):
            return len(sample["Target"])>10 and len(sample["Drug"])>1
        dataset = dataset.filter(filter_invalid)
        self.raw_dataset_samples = [s for s in dataset]
        print(f"âœ… è¿‡æ»¤åæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(self.raw_dataset_samples)}")

        protein_proc2raw = {self.preprocess_protein(p):p for p in dataset["Target"]}
        ligand_proc2raw = {self.preprocess_ligand(l):l for l in dataset["Drug"]}

        unique_proteins = list(protein_proc2raw.keys())[:max_proteins]
        unique_ligands = list(ligand_proc2raw.keys())[:max_ligands]
        print(f"ğŸ“Š æ„å»ºç´¢å¼• - è›‹ç™½æ•°ï¼š{len(unique_proteins)} | å°åˆ†å­æ•°ï¼š{len(unique_ligands)}")

        protein_embs = []
        for i in tqdm(range(0, len(unique_proteins), self.batch_size), desc="ç¼–ç è›‹ç™½åºåˆ—"):
            batch_seqs = unique_proteins[i:i+self.batch_size]
            batch_embs = self.encode_protein_batch(batch_seqs)
            protein_embs.append(batch_embs)
        protein_embs = np.concatenate(protein_embs, axis=0)

        ligand_embs = []
        for i in tqdm(range(0, len(unique_ligands), self.batch_size), desc="ç¼–ç å°åˆ†å­SMILES"):
            batch_smiles = unique_ligands[i:i+self.batch_size]
            batch_embs = self.encode_ligand_batch(batch_smiles)
            ligand_embs.append(batch_embs)
        ligand_embs = np.concatenate(ligand_embs, axis=0)

        self.protein_id2seq = {i:{"processed":s,"raw":protein_proc2raw[s]} for i,s in enumerate(unique_proteins)}
        self.ligand_id2smiles = {i:{"processed":s,"raw":ligand_proc2raw[s]} for i,s in enumerate(unique_ligands)}

        self.protein_index = faiss.IndexFlatIP(256)
        self.protein_index.add(protein_embs)
        self.ligand_index = faiss.IndexFlatIP(256)
        self.ligand_index.add(ligand_embs)

        print(f"âœ… FAISSç´¢å¼•æ„å»ºå®Œæˆ - è›‹ç™½ç´¢å¼•æ•°ï¼š{self.protein_index.ntotal} | å°åˆ†å­ç´¢å¼•æ•°ï¼š{self.ligand_index.ntotal}")

    def retrieve_ligands(self, protein_seq, top_k=10):
        query_emb = self.encode_protein_batch([protein_seq])[0].reshape(1, -1)
        distances, indices = self.ligand_index.search(query_emb, top_k)

        max_sim = np.max(distances[0]) if len(distances[0]) > 0 else 1.0
        min_sim = np.min(distances[0]) if len(distances[0]) > 0 else 0.0
        norm_distances = (distances[0] - min_sim) / (max_sim - min_sim + 1e-8)

        results = []
        for i, idx in enumerate(indices[0]):
            if 0 <= idx < len(self.ligand_id2smiles):
                info = self.ligand_id2smiles[idx]
                results.append({
                    "smiles": info["raw"],
                    "smiles_processed": info["processed"],
                    "similarity": float(norm_distances[i])
                })
        return results

    def retrieve_proteins(self, ligand_smiles, top_k=10):
        query_emb = self.encode_ligand_batch([ligand_smiles])[0].reshape(1, -1)
        distances, indices = self.protein_index.search(query_emb, top_k)

        max_sim = np.max(distances[0]) if len(distances[0]) > 0 else 1.0
        min_sim = np.min(distances[0]) if len(distances[0]) > 0 else 0.0
        norm_distances = (distances[0] - min_sim) / (max_sim - min_sim + 1e-8)

        results = []
        for i, idx in enumerate(indices[0]):
            if 0 <= idx < len(self.protein_id2seq):
                info = self.protein_id2seq[idx]
                results.append({
                    "protein_seq": info["raw"],
                    "protein_processed": info["processed"],
                    "similarity": float(norm_distances[i])
                })
        return results


if __name__ == "__main__":
    CHECKPOINT_PATH = "../model/checkpoints/saprot_clip_tower_best.ckpt"
    SAPROT_MODEL_DIR = "../models/SaProt_1.3B_AFDB_OMG_NCBI"
    CHEMBERTA_MODEL_NAME = "DeepChem/ChemBERTa-77M-MLM"
    TEMPERATURE_SCALE = 0.5
    DEVICE = "cuda:0"
    BATCH_SIZE = 64

    # è·¯å¾„æ ¡éªŒ
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"âŒ é”™è¯¯ï¼šckptæ–‡ä»¶ä¸å­˜åœ¨ - {CHECKPOINT_PATH}")
        sys.exit(1)
    if not os.path.exists(SAPROT_MODEL_DIR):
        print(f"âŒ é”™è¯¯ï¼šSaProtæ¨¡å‹ç›®å½•ä¸å­˜åœ¨ - {SAPROT_MODEL_DIR}")
        sys.exit(1)

    try:
        retriever = ProteinLigandRetriever(
            checkpoint_path=CHECKPOINT_PATH,
            saprot_model_dir=SAPROT_MODEL_DIR,
            chemberta_model_name=CHEMBERTA_MODEL_NAME,
            device=DEVICE,
            temperature_scale=TEMPERATURE_SCALE,
            batch_size=BATCH_SIZE
        )
    except Exception as e:
        print(f"âŒ æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

    try:
        from datasets import load_dataset
        ds = load_dataset("BALM/BALM-benchmark", "BindingDB_filtered")
        retriever.build_indexes_from_dataset(ds, max_proteins=2000, max_ligands=8000)
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½/ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

    print("\n" + "="*50)
    print("ğŸ“ å¼€å§‹æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("="*50)

    if len(retriever.raw_dataset_samples) > 0:
        test_protein = retriever.raw_dataset_samples[0]["Target"]
        test_smiles = retriever.raw_dataset_samples[0]["Drug"]
    else:
        test_protein = "MSHHWGYGKHNGPEHWHKDFPIAKGERQSPVDIDTHTAKYDPSLKPLSVSYDQATSLRIL"
        test_smiles = "Cc1ccc(CNS(=O)(=O)c2ccc(S(N)(=O)=O)s2)cc1"

    print(f"\nğŸ” æµ‹è¯•1ï¼šè›‹ç™½åºåˆ— â†’ å°åˆ†å­æ£€ç´¢")
    print(f"æŸ¥è¯¢è›‹ç™½ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{test_protein[:50]}...")
    ligand_results = retriever.retrieve_ligands(test_protein, top_k=5)
    for i, res in enumerate(ligand_results):
        print(f"  Top{i+1} - SMILESï¼š{res['smiles'][:50]} | ç›¸ä¼¼åº¦ï¼š{res['similarity']:.4f}")


    print(f"\nğŸ” æµ‹è¯•2ï¼šå°åˆ†å­SMILES â†’ è›‹ç™½æ£€ç´¢")
    print(f"æŸ¥è¯¢SMILESï¼š{test_smiles}")
    protein_results = retriever.retrieve_proteins(test_smiles, top_k=5)
    for i, res in enumerate(protein_results):
        print(f"  Top{i+1} - è›‹ç™½ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{res['protein_seq'][:50]}... | ç›¸ä¼¼åº¦ï¼š{res['similarity']:.4f}")

    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")